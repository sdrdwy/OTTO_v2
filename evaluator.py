#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
import json
import os
import time
import logging
from typing import Generator, Dict, List, Any, Tuple
from pathlib import Path

import dashscope
from dashscope import Generation
# æ³¨æ„ï¼šå·²ç§»é™¤ `from dashscope.exceptions import DashScopeException`
from tqdm import tqdm

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler("evaluation_errors.log"), logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# è¯„ä¼°ç»´åº¦ï¼ˆä¿æŒä¸å˜ï¼‰
EVAL_CRITERIA = {
    "å…³é”®ä¿¡æ¯è®°å¿†å‡†ç¡®æ€§": "å¤šè½®å¯¹è¯ä¸­ï¼Œå¯¹ç”¨æˆ·æåŠçš„æ ¸å¿ƒä¿¡æ¯ï¼ˆå§“å / éœ€æ±‚ / åå¥½ / å†å²çº¦å®šï¼‰è®°å¿†æ— åå·®ã€æ— é—æ¼",
    "æ— è™šå‡è®°å¿†ä¸æ··æ·†": "ä¸ç¼–é€ æœªæåŠçš„ä¿¡æ¯ï¼Œä¸æ··æ·†ä¸åŒç”¨æˆ· / ä¸åŒæ—¶æ®µçš„è®°å¿†",
    "äººè®¾ç‰¹è´¨è·¨è½®ç¨³å®šæ€§": "å¤šè½®å¯¹è¯ä¸­ï¼Œæ ¸å¿ƒç‰¹è´¨å§‹ç»ˆç»Ÿä¸€ï¼Œæ— å‰åçŸ›ç›¾",
    "è·¨åœºæ™¯äººè®¾é€‚é…è¿è´¯æ€§": "å¤šè½®åˆ‡æ¢åœºæ™¯æ—¶ï¼Œäººè®¾ç‰¹è´¨ä¸å˜ï¼Œä»…åšåœºæ™¯é€‚é…",
    "è¯­è¨€é£æ ¼è·¨è½®ç»Ÿä¸€æ€§": "å¤šè½®å¯¹è¯çš„è¯æ±‡ã€å¥å¼ã€è¯­æ°”åŠ©è¯ä½¿ç”¨é•¿æœŸç»Ÿä¸€",
    "æƒ…æ„ŸåŸºè°ƒè·¨è½®ç¨³å®šæ€§": "å¤šè½®å¯¹è¯çš„æƒ…æ„Ÿå€¾å‘ã€å¼ºåº¦å§‹ç»ˆä¸äººè®¾åŒ¹é…"
}


def load_conversations(path: str) -> List[Dict[str, Any]]:
    """
    åŠ è½½å¯¹è¯æ•°æ®ï¼Œå…¼å®¹ä¸¤ç§æ ¼å¼ï¼š
    1. å•ä¸ªå¯¹è¯å¯¹è±¡ï¼ˆå¤šè¡Œ JSONï¼‰
    2. å¯¹è¯å¯¹è±¡æ•°ç»„ï¼ˆ[ {...}, {...} ]ï¼‰
    ä¸å†ä½¿ç”¨ Generatorï¼Œç›´æ¥è¿”å› listã€‚
    """
    try:
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if isinstance(data, dict):
            # å•ä¸ªå¯¹è¯å¯¹è±¡
            logger.info("æ£€æµ‹åˆ°å•ä¸ªå¯¹è¯å¯¹è±¡")
            return [data]
        elif isinstance(data, list):
            # å¯¹è¯å¯¹è±¡æ•°ç»„
            logger.info(f"æ£€æµ‹åˆ°åŒ…å« {len(data)} ä¸ªå¯¹è¯çš„æ•°ç»„")
            return data
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„JSONæ ¹ç±»å‹: {type(data)}")
    except json.JSONDecodeError as e:
        logger.error(f"JSONè§£æå¤±è´¥ï¼ˆæ–‡ä»¶æ•´ä½“ï¼‰: {e}")
        raise


def save_jsonl(data: List[Dict[str, Any]], path: str, overwrite: bool = False) -> None:
    """ä¿å­˜ä¸ºJSONLæ ¼å¼"""
    output_path = Path(path)
    if output_path.exists() and not overwrite:
        raise FileExistsError(f"è¾“å‡ºæ–‡ä»¶ {path} å·²å­˜åœ¨ï¼Œå¦‚éœ€è¦†ç›–è¯·æ·»åŠ  --overwrite å‚æ•°")
    with open(path, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    logger.info(f"ç»“æœå·²ä¿å­˜è‡³: {path}")


def format_persona(persona: Dict[str, Any]) -> str:
    """å°† persona å­—å…¸è½¬ä¸ºå¯è¯»æ–‡æœ¬"""
    lines = [f"å§“å: {persona.get('name', 'æœªçŸ¥')}"]
    lines.append(f"äººè®¾: {persona.get('persona', 'æœªå®šä¹‰')}")
    lines.append(f"å¯¹è¯é£æ ¼: {persona.get('å¯¹è¯é£æ ¼', 'æœªå®šä¹‰')}")
    habits = persona.get("æ—¥å¸¸ä¹ æƒ¯", {})
    if habits:
        likes = ', '.join(habits.get("åå¥½çš„æ´»åŠ¨", []))
        dislikes = ', '.join(habits.get("ä¸å–œæ¬¢çš„æ´»åŠ¨", []))
        lines.append(f"åå¥½æ´»åŠ¨: {likes}")
        lines.append(f"åŒæ¶æ´»åŠ¨: {dislikes}")
    return "\n".join(lines)


def truncate_dialogue_by_tokens(
    dialogue: List[Dict[str, str]],
    max_input_tokens: int = 16000,
    reserve_tokens: int = 1024
) -> List[Dict[str, str]]:
    """
    ä»åå¾€å‰æˆªæ–­å¯¹è¯ï¼Œç¡®ä¿æ€»è¾“å…¥ token ä¸è¶…é™ï¼ˆç”¨å­—ç¬¦æ•°ä¿å®ˆä¼°è®¡ tokenï¼‰
    """
    available_tokens = max_input_tokens - reserve_tokens
    truncated = []
    current_tokens = 0

    for turn in reversed(dialogue):
        # æ„é€ è¯¥è½®æ–‡æœ¬ï¼ˆä¸ prompt ä¸­æ ¼å¼ä¸€è‡´ï¼‰
        text = f"{turn['speaker']}ï¼š{turn['message']}\n"
        turn_tokens = len(text) // 3  # ä¿å®ˆä¼°è®¡ï¼š1 token â‰ˆ 3~4 å­—ç¬¦

        if current_tokens + turn_tokens > available_tokens:
            break
        truncated.append(turn)
        current_tokens += turn_tokens

    return list(reversed(truncated))  # æ¢å¤æ—¶é—´é¡ºåº


def build_judge_prompt(
    full_dialogue: List[Dict[str, str]],
    target_speaker: str,
    speaker_persona: Dict[str, Any],
    max_input_tokens: int = 16000
) -> str:
    """æ„é€ è¯„ä¼° promptï¼ˆæ”¯æŒå¤šäººã€å¸¦æˆªæ–­ï¼‰"""
    truncated_dialogue = truncate_dialogue_by_tokens(full_dialogue, max_input_tokens)
    
    dialog_text = ""
    for turn in truncated_dialogue:
        dialog_text += f"{turn['speaker']}ï¼š{turn['message']}\n"

    persona_text = format_persona(speaker_persona)

    prompt = (
        "ä½ æ˜¯ä¸€ä¸ªå¯¹è¯è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚è¯·è¯„ä¼°è§’è‰²ã€Œ{target_speaker}ã€åœ¨ä»¥ä¸‹å¯¹è¯ä¸­çš„æ•´ä½“è¡¨ç°ã€‚\n"
        "è¯„ä¼°ç»´åº¦åŒ…æ‹¬ï¼šå…³é”®ä¿¡æ¯è®°å¿†å‡†ç¡®æ€§ã€æ— è™šå‡è®°å¿†ä¸æ··æ·†ã€äººè®¾ç‰¹è´¨è·¨è½®ç¨³å®šæ€§ã€"
        "è·¨åœºæ™¯äººè®¾é€‚é…è¿è´¯æ€§ã€è¯­è¨€é£æ ¼è·¨è½®ç»Ÿä¸€æ€§ã€æƒ…æ„ŸåŸºè°ƒè·¨è½®ç¨³å®šæ€§ã€‚\n"
        "æ¯ä¸ªç»´åº¦è¯„åˆ†èŒƒå›´ 0-10 åˆ†ï¼ˆ10=å®Œç¾ç¬¦åˆï¼Œ0=ä¸¥é‡è¿èƒŒï¼‰ï¼Œå¹¶ç»™å‡ºç®€è¦ä¸­æ–‡è¯„è¯­ï¼ˆ1-30å­—ï¼‰ã€‚\n"
        "è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼Œä¸è¦ä»»ä½•é¢å¤–å†…å®¹ï¼ˆå¦‚è§£é‡Šã€æ¢è¡Œã€å¤‡æ³¨ï¼‰ï¼š\n"
        "{{\n"
        "  \"å…³é”®ä¿¡æ¯è®°å¿†å‡†ç¡®æ€§\": {{\"score\": 8, \"comment\": \"æ ¸å¿ƒä¿¡æ¯è®°å¿†å‡†ç¡®ï¼Œæ— é—æ¼\"}},\n"
        "  \"æ— è™šå‡è®°å¿†ä¸æ··æ·†\": {{\"score\": 9, \"comment\": \"æ— ç¼–é€ ä¿¡æ¯ï¼Œæœªæ··æ·†ç”¨æˆ·è®°å¿†\"}},\n"
        "  \"äººè®¾ç‰¹è´¨è·¨è½®ç¨³å®šæ€§\": {{\"score\": 7, \"comment\": \"äººè®¾æ ¸å¿ƒç‰¹è´¨ç»Ÿä¸€ï¼Œæ— çŸ›ç›¾\"}},\n"
        "  \"è·¨åœºæ™¯äººè®¾é€‚é…è¿è´¯æ€§\": {{\"score\": 8, \"comment\": \"åœºæ™¯åˆ‡æ¢åäººè®¾æœªå˜ï¼Œé€‚é…åˆç†\"}},\n"
        "  \"è¯­è¨€é£æ ¼è·¨è½®ç»Ÿä¸€æ€§\": {{\"score\": 9, \"comment\": \"è¯æ±‡å’Œå¥å¼ä¿æŒä¸€è‡´ï¼Œé£æ ¼ç»Ÿä¸€\"}},\n"
        "  \"æƒ…æ„ŸåŸºè°ƒè·¨è½®ç¨³å®šæ€§\": {{\"score\": 8, \"comment\": \"æƒ…æ„Ÿå€¾å‘ä¸äººè®¾åŒ¹é…ï¼Œæ— æ³¢åŠ¨\"}}\n"
        "}}\n\n"
        "äººç‰©äººè®¾å¦‚ä¸‹ï¼š\n"
        "{persona_text}\n\n"
        "å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆå¯èƒ½å·²æˆªæ–­ä»¥é€‚åº”æ¨¡å‹è¾“å…¥é™åˆ¶ï¼‰ï¼š\n"
        "{dialog_text}\n"
        "è¯·è¯„ä¼°ã€Œ{target_speaker}ã€çš„æ•´ä½“è¡¨ç°ï¼š"
    ).format(
        target_speaker=target_speaker,
        persona_text=persona_text,
        dialog_text=dialog_text
    )
    return prompt


def validate_evaluation(eval_res: Dict[str, Any]) -> bool:
    """éªŒè¯è¯„ä¼°ç»“æœåˆæ³•æ€§"""
    required_keys = list(EVAL_CRITERIA.keys())
    for key in required_keys:
        if key not in eval_res:
            logger.error(f"ç¼ºå°‘è¯„ä¼°ç»´åº¦: {key}")
            return False
        dim_data = eval_res[key]
        if not isinstance(dim_data, dict) or "score" not in dim_data or "comment" not in dim_data:
            logger.error(f"ç»´åº¦{key}æ ¼å¼é”™è¯¯")
            return False
        score = dim_data["score"]
        if not isinstance(score, (int, float)) or not (0 <= score <= 10):
            logger.error(f"ç»´åº¦{key}è¯„åˆ†å¼‚å¸¸: {score}")
            return False
        comment = dim_data["comment"].strip()
        if not comment:
            logger.error(f"ç»´åº¦{key}è¯„è¯­ä¸ºç©º")
            return False
        if len(comment) > 30:
            logger.warning(f"ç»´åº¦{key}è¯„è¯­è¿‡é•¿: {comment}")
    return True


def evaluate_speaker_in_dialogue(
    prompt: str,
    model: str = "qwen-plus",
    retry_times: int = 3,
    base_sleep: float = 0.5
) -> Dict[str, Any]:
    """è¯„ä¼°å•ä¸ªè§’è‰²åœ¨å¯¹è¯ä¸­çš„è¡¨ç°ï¼ˆå¸¦é‡è¯•ï¼‰"""
    default_error = {
        k: {"score": -1, "comment": "[è¯„ä¼°å¤±è´¥] æœªçŸ¥é”™è¯¯"} for k in EVAL_CRITERIA.keys()
    }
    
    for retry in range(retry_times):
        try:
            response = Generation.call(
                model=model,
                prompt=prompt,
                result_format="text",
                max_tokens=1024,
                temperature=0.2,
                top_p=0.9,
                timeout=30
            )
            
            if response.status_code != 200:
                error_msg = f"APIå“åº”é”™è¯¯: {response.code} - {response.message}"
                logger.error(error_msg)
                if retry < retry_times - 1:
                    time.sleep(base_sleep * (2 ** retry))
                    continue
                return {
                    "detailed_evaluation": {k: {"score": -1, "comment": error_msg} for k in EVAL_CRITERIA.keys()},
                    "raw_output": "",
                    "status": "api_error"
                }
            
            generated = response.output.text.strip()
            if not generated:
                error_msg = "APIè¿”å›ç©ºå†…å®¹"
                logger.error(error_msg)
                if retry < retry_times - 1:
                    time.sleep(base_sleep * (2 ** retry))
                    continue
                return {
                    "detailed_evaluation": {k: {"score": -1, "comment": error_msg} for k in EVAL_CRITERIA.keys()},
                    "raw_output": generated,
                    "status": "empty_output"
                }
            
            eval_res = json.loads(generated)
            if validate_evaluation(eval_res):
                logger.info("è¯„ä¼°ç»“æœéªŒè¯é€šè¿‡")
                return {
                    "detailed_evaluation": eval_res,
                    "raw_output": generated,
                    "status": "success"
                }
            else:
                error_msg = "è¯„ä¼°ç»“æœæ ¼å¼éªŒè¯å¤±è´¥"
                logger.error(f"{error_msg}ï¼ŒåŸå§‹è¾“å‡º: {generated}")
                if retry < retry_times - 1:
                    time.sleep(base_sleep * (2 ** retry))
                    continue
                return {
                    "detailed_evaluation": {k: {"score": -1, "comment": error_msg} for k in EVAL_CRITERIA.keys()},
                    "raw_output": generated,
                    "status": "validation_failed"
                }
        
        # === ä¿®æ”¹ç‚¹ï¼šä¸å†æ•è· DashScopeExceptionï¼Œæ”¹ç”¨é€šç”¨ Exception ===
        except Exception as e:
            # åˆ¤æ–­æ˜¯å¦æ˜¯ DashScope ç›¸å…³çš„é”™è¯¯ï¼ˆé€šè¿‡å±æ€§æ¨æµ‹ï¼‰
            error_str = str(e)
            if "dashscope" in error_str.lower() or "api" in error_str.lower() or "quota" in error_str.lower():
                error_msg = f"DashScope APIå¼‚å¸¸: {error_str}"
            else:
                error_msg = f"æœªçŸ¥å¼‚å¸¸: {error_str}"
            
            logger.error(error_msg, exc_info=True)
            if retry < retry_times - 1:
                time.sleep(base_sleep * (2 ** retry))
                continue
            return {
                "detailed_evaluation": {k: {"score": -1, "comment": error_msg} for k in EVAL_CRITERIA.keys()},
                "raw_output": "",
                "status": "dashscope_or_unknown_error"
            }
        # =============================================================

    logger.error(f"æ‰€æœ‰{retry_times}æ¬¡é‡è¯•å‡å¤±è´¥")
    return {
        "detailed_evaluation": default_error,
        "raw_output": "",
        "status": "all_retries_failed"
    }


def main():
    INPUT_FILE = "logs.jsonl"          # è¾“å…¥æ–‡ä»¶è·¯å¾„
    OUTPUT_FILE = "evaluation_results.jsonl"      # è¾“å‡ºæ–‡ä»¶è·¯å¾„
    PERSONA_MAP_PATH = "personas.json" # äººè®¾æ˜ å°„æ–‡ä»¶
    MODEL = "qwen-plus"                # è¯„ä¼°æ¨¡å‹
    API_KEY = "sk-6ad3d58adcb44469b6020722bd945ad6"
    if not API_KEY:
        raise ValueError("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY")
    dashscope.api_key = API_KEY

    MAX_INPUT_TOKENS = 16000
    RETRY_TIMES = 3
    BASE_SLEEP = 0.5

    with open(PERSONA_MAP_PATH, 'r', encoding='utf-8') as f:
        persona_map = json.load(f)
    logger.info(f"å·²åŠ è½½ {len(persona_map)} ä¸ªè§’è‰²çš„äººè®¾")

    all_results = []
    conversations = load_conversations(INPUT_FILE)  # ç›´æ¥åŠ è½½ä¸º list

    total_conv = len(conversations)

    # å¤„ç†æ¯ä¸ªå¯¹è¯ï¼ˆå®Œå…¨å¤ç”¨åŸé€»è¾‘ï¼‰
    for conv_idx, conversation in enumerate(tqdm(conversations, total=total_conv, desc="å¤„ç†å¯¹è¯")):
        conv_id = conversation.get("id", f"conv_{conv_idx}")
        dialogue_history = conversation.get("dialogue_history", [])
        
        if not isinstance(dialogue_history, list) or len(dialogue_history) == 0:
            logger.warning(f"å¯¹è¯ {conv_id} æ— æœ‰æ•ˆå¯¹è¯å†å²ï¼Œè·³è¿‡")
            continue

        # æå–æ‰€æœ‰å‘è¨€è€…
        speakers = set(turn.get("speaker") for turn in dialogue_history if turn.get("speaker"))
        if not speakers:
            logger.warning(f"å¯¹è¯ {conv_id} æ— æœ‰æ•ˆå‘è¨€è€…ï¼Œè·³è¿‡")
            continue

        speaker_evaluations = {}
        for speaker in speakers:
            if speaker not in persona_map:
                logger.warning(f"è§’è‰² {speaker} æœªåœ¨ persona-map ä¸­å®šä¹‰ï¼Œè·³è¿‡è¯„ä¼°")
                continue
            
            logger.info(f"è¯„ä¼°å¯¹è¯ {conv_id} ä¸­è§’è‰² {speaker}")
            prompt = build_judge_prompt(
                full_dialogue=dialogue_history,
                target_speaker=speaker,
                speaker_persona=persona_map[speaker],
                max_input_tokens=MAX_INPUT_TOKENS
            )
            eval_res = evaluate_speaker_in_dialogue(
                prompt,
                model=MODEL,
                retry_times=RETRY_TIMES,
                base_sleep=BASE_SLEEP
            )
            speaker_evaluations[speaker] = eval_res
            
            if eval_res["status"] == "success":
                time.sleep(BASE_SLEEP)  # ä»…æˆåŠŸåé™æµ

        all_results.append({
            "conversation_id": conv_id,
            # "dialogue_history": dialogue_history,
            "speaker_evaluations": speaker_evaluations,
            "total_evaluated_speakers": len(speaker_evaluations)
        })

    # ä¿å­˜ç»“æœï¼ˆè¦†ç›–æ¨¡å¼ï¼‰
    save_jsonl(all_results, OUTPUT_FILE, overwrite=True)
    
    # ç»Ÿè®¡
    total_conv = len(all_results)
    total_speakers = sum(conv["total_evaluated_speakers"] for conv in all_results)
    success_speakers = sum(
        1 for conv in all_results
        for eval_res in conv["speaker_evaluations"].values()
        if eval_res["status"] == "success"
    )
    logger.info(f"âœ… è¯„ä¼°å®Œæˆï¼")
    logger.info(f"ğŸ“Š ç»Ÿè®¡ï¼šå¤„ç†å¯¹è¯æ•°={total_conv}ï¼Œè¯„ä¼°è§’è‰²æ•°={total_speakers}ï¼ŒæˆåŠŸ={success_speakers}ï¼Œå¤±è´¥={total_speakers - success_speakers}")


if __name__ == "__main__":
    main()